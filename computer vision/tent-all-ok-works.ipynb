{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install git+https://github.com/RobustBench/robustbench.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-01-28T03:28:45.162345Z","iopub.execute_input":"2025-01-28T03:28:45.162732Z","iopub.status.idle":"2025-01-28T03:29:06.125579Z","shell.execute_reply.started":"2025-01-28T03:28:45.162694Z","shell.execute_reply":"2025-01-28T03:29:06.124625Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/RobustBench/robustbench.git\n  Cloning https://github.com/RobustBench/robustbench.git to /tmp/pip-req-build-vkft6ou5\n  Running command git clone --filter=blob:none --quiet https://github.com/RobustBench/robustbench.git /tmp/pip-req-build-vkft6ou5\n  Resolved https://github.com/RobustBench/robustbench.git to commit 46a91f44524133b2cd8f721ec7e73ecb63f17fc8\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting autoattack@ git+https://github.com/fra31/auto-attack.git@a39220048b3c9f2cca9a4d3a54604793c68eca7e#egg=autoattack (from robustbench==1.1)\n  Cloning https://github.com/fra31/auto-attack.git (to revision a39220048b3c9f2cca9a4d3a54604793c68eca7e) to /tmp/pip-install-g7gphgae/autoattack_ce7ac0e7a73a43eb921f816db5059706\n  Running command git clone --filter=blob:none --quiet https://github.com/fra31/auto-attack.git /tmp/pip-install-g7gphgae/autoattack_ce7ac0e7a73a43eb921f816db5059706\n  Running command git rev-parse -q --verify 'sha^a39220048b3c9f2cca9a4d3a54604793c68eca7e'\n  Running command git fetch -q https://github.com/fra31/auto-attack.git a39220048b3c9f2cca9a4d3a54604793c68eca7e\n  Resolved https://github.com/fra31/auto-attack.git to commit a39220048b3c9f2cca9a4d3a54604793c68eca7e\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch>=1.7.1 in /opt/conda/lib/python3.10/site-packages (from robustbench==1.1) (2.1.2)\nRequirement already satisfied: torchvision>=0.8.2 in /opt/conda/lib/python3.10/site-packages (from robustbench==1.1) (0.16.2)\nCollecting torchdiffeq (from robustbench==1.1)\n  Downloading torchdiffeq-0.2.5-py3-none-any.whl.metadata (440 bytes)\nCollecting geotorch (from robustbench==1.1)\n  Downloading geotorch-0.3.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: requests>=2.25.0 in /opt/conda/lib/python3.10/site-packages (from robustbench==1.1) (2.32.3)\nRequirement already satisfied: numpy>=1.19.4 in /opt/conda/lib/python3.10/site-packages (from robustbench==1.1) (1.26.4)\nRequirement already satisfied: Jinja2~=3.1.2 in /opt/conda/lib/python3.10/site-packages (from robustbench==1.1) (3.1.2)\nRequirement already satisfied: tqdm>=4.56.1 in /opt/conda/lib/python3.10/site-packages (from robustbench==1.1) (4.66.4)\nRequirement already satisfied: pandas>=1.3.5 in /opt/conda/lib/python3.10/site-packages (from robustbench==1.1) (2.2.1)\nCollecting timm==1.0.9 (from robustbench==1.1)\n  Downloading timm-1.0.9-py3-none-any.whl.metadata (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting gdown==5.1.0 (from robustbench==1.1)\n  Downloading gdown-5.1.0-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from robustbench==1.1) (6.0.1)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown==5.1.0->robustbench==1.1) (4.12.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown==5.1.0->robustbench==1.1) (3.13.1)\nRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (from timm==1.0.9->robustbench==1.1) (0.23.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm==1.0.9->robustbench==1.1) (0.4.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2~=3.1.2->robustbench==1.1) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.3.5->robustbench==1.1) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.3.5->robustbench==1.1) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.3.5->robustbench==1.1) (2023.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->robustbench==1.1) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->robustbench==1.1) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->robustbench==1.1) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->robustbench==1.1) (2024.2.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->robustbench==1.1) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->robustbench==1.1) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->robustbench==1.1) (3.2.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->robustbench==1.1) (2024.3.1)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.8.2->robustbench==1.1) (9.5.0)\nRequirement already satisfied: scipy>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from torchdiffeq->robustbench==1.1) (1.11.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.3.5->robustbench==1.1) (1.16.0)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown==5.1.0->robustbench==1.1) (2.5)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm==1.0.9->robustbench==1.1) (21.3)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown==5.1.0->robustbench==1.1) (1.7.1)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.7.1->robustbench==1.1) (1.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub->timm==1.0.9->robustbench==1.1) (3.1.1)\nDownloading gdown-5.1.0-py3-none-any.whl (17 kB)\nDownloading timm-1.0.9-py3-none-any.whl (2.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading geotorch-0.3.0-py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torchdiffeq-0.2.5-py3-none-any.whl (32 kB)\nBuilding wheels for collected packages: robustbench, autoattack\n  Building wheel for robustbench (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for robustbench: filename=robustbench-1.1-py3-none-any.whl size=165964 sha256=2666778198511d719ae269500bc5814ed85235b6b7cdacfa1619249f562290b0\n  Stored in directory: /tmp/pip-ephem-wheel-cache-g9dkykw0/wheels/a6/95/83/6461b55d0c3f761c07216e248dfa511f4d780da4c1e00294b8\n  Building wheel for autoattack (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for autoattack: filename=autoattack-0.1-py3-none-any.whl size=36230 sha256=fb4f73ded27fcee591d42b5eb96e3f6e5b3422a6641e74cdfa1a1b6bfa18db03\n  Stored in directory: /root/.cache/pip/wheels/72/59/98/10a8eb862dabff57be1d1e3b1a3664acc78da7667d2c835584\nSuccessfully built robustbench autoattack\nInstalling collected packages: autoattack, torchdiffeq, geotorch, gdown, timm, robustbench\n  Attempting uninstall: timm\n    Found existing installation: timm 1.0.3\n    Uninstalling timm-1.0.3:\n      Successfully uninstalled timm-1.0.3\nSuccessfully installed autoattack-0.1 gdown-5.1.0 geotorch-0.3.0 robustbench-1.1 timm-1.0.9 torchdiffeq-0.2.5\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from copy import deepcopy\n\nimport torch\nimport torch.nn as nn\nimport torch.jit\n\n\nclass Tent(nn.Module):\n    \"\"\"Tent adapts a model by entropy minimization during testing.\n\n    Once tented, a model adapts itself by updating on every forward.\n    \"\"\"\n    def __init__(self, model, optimizer, steps=1, episodic=False):\n        super().__init__()\n        self.model = model\n        self.optimizer = optimizer\n        self.steps = steps\n        assert steps > 0, \"tent requires >= 1 step(s) to forward and update\"\n        self.episodic = episodic\n\n        # note: if the model is never reset, like for continual adaptation,\n        # then skipping the state copy would save memory\n        self.model_state, self.optimizer_state = \\\n            copy_model_and_optimizer(self.model, self.optimizer)\n\n    def forward(self, x):\n        if self.episodic:\n            self.reset()\n\n        for _ in range(self.steps):\n            outputs = forward_and_adapt(x, self.model, self.optimizer)\n\n        return outputs\n\n    def reset(self):\n        if self.model_state is None or self.optimizer_state is None:\n            raise Exception(\"cannot reset without saved model/optimizer state\")\n        load_model_and_optimizer(self.model, self.optimizer,\n                                 self.model_state, self.optimizer_state)\n\n\n@torch.jit.script\ndef softmax_entropy(x: torch.Tensor) -> torch.Tensor:\n    \"\"\"Entropy of softmax distribution from logits.\"\"\"\n    return -(x.softmax(1) * x.log_softmax(1)).sum(1)\n\n\n@torch.enable_grad()  # ensure grads in possible no grad context for testing\ndef forward_and_adapt(x, model, optimizer):\n    \"\"\"Forward and adapt model on batch of data.\n\n    Measure entropy of the model prediction, take gradients, and update params.\n    \"\"\"\n    # forward\n    outputs = model(x)\n    # adapt\n    loss = softmax_entropy(outputs).mean(0)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    return outputs\n\n\ndef collect_params(model):\n    \"\"\"Collect the affine scale + shift parameters from batch norms.\n\n    Walk the model's modules and collect all batch normalization parameters.\n    Return the parameters and their names.\n\n    Note: other choices of parameterization are possible!\n    \"\"\"\n    params = []\n    names = []\n    for nm, m in model.named_modules():\n        if isinstance(m, nn.BatchNorm2d):\n            for np, p in m.named_parameters():\n                if np in ['weight', 'bias']:  # weight is scale, bias is shift\n                    params.append(p)\n                    names.append(f\"{nm}.{np}\")\n    return params, names\n\n\ndef copy_model_and_optimizer(model, optimizer):\n    \"\"\"Copy the model and optimizer states for resetting after adaptation.\"\"\"\n    model_state = deepcopy(model.state_dict())\n    optimizer_state = deepcopy(optimizer.state_dict())\n    return model_state, optimizer_state\n\n\ndef load_model_and_optimizer(model, optimizer, model_state, optimizer_state):\n    \"\"\"Restore the model and optimizer states from copies.\"\"\"\n    model.load_state_dict(model_state, strict=True)\n    optimizer.load_state_dict(optimizer_state)\n\n\ndef configure_model(model):\n    \"\"\"Configure model for use with tent.\"\"\"\n    # train mode, because tent optimizes the model to minimize entropy\n    model.train()\n    # disable grad, to (re-)enable only what tent updates\n    model.requires_grad_(False)\n    # configure norm for tent updates: enable grad + force batch statisics\n    for m in model.modules():\n        if isinstance(m, nn.BatchNorm2d):\n            m.requires_grad_(True)\n            # force use of batch stats in train and eval modes\n            m.track_running_stats = False\n            m.running_mean = None\n            m.running_var = None\n    return model\n\n\ndef check_model(model):\n    \"\"\"Check model for compatability with tent.\"\"\"\n    is_training = model.training\n    assert is_training, \"tent needs train mode: call model.train()\"\n    param_grads = [p.requires_grad for p in model.parameters()]\n    has_any_params = any(param_grads)\n    has_all_params = all(param_grads)\n    assert has_any_params, \"tent needs params to update: \" \\\n                           \"check which require grad\"\n    assert not has_all_params, \"tent should not update all params: \" \\\n                               \"check which require grad\"\n    has_bn = any([isinstance(m, nn.BatchNorm2d) for m in model.modules()])\n    assert has_bn, \"tent needs normalization for its optimization\"","metadata":{"execution":{"iopub.status.busy":"2025-01-28T03:29:06.127430Z","iopub.execute_input":"2025-01-28T03:29:06.127719Z","iopub.status.idle":"2025-01-28T03:29:09.303242Z","shell.execute_reply.started":"2025-01-28T03:29:06.127695Z","shell.execute_reply":"2025-01-28T03:29:09.302513Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cfg_MODEL_ARCH = 'Standard'\n\n# Choice of (source, norm, tent)\n# - source: baseline without adaptation\n# - norm: test-time normalization\n# - tent: test-time entropy minimization (ours)\ncfg_MODEL_ADAPTATION = 'source'\n\n# By default tent is online, with updates persisting across batches.\n# To make adaptation episodic, and reset the model for each batch, choose True.\ncfg_MODEL_EPISODIC = False\n\n# ----------------------------- Corruption options -------------------------- #\n#cfg_CORRUPTION = CfgNode()\n\n# Dataset for evaluation\ncfg_CORRUPTION_DATASET = 'cifar10'\n\n# Check https://github.com/hendrycks/robustness for corruption details\ncfg_CORRUPTION_TYPE = ['gaussian_noise', 'shot_noise', 'impulse_noise',\n                      'defocus_blur', 'glass_blur', 'motion_blur', 'zoom_blur',\n                      'snow', 'frost', 'fog', 'brightness', 'contrast',\n                      'elastic_transform', 'pixelate', 'jpeg_compression']\ncfg_CORRUPTION_SEVERITY = [5, 4, 3, 2, 1]\n\n# Number of examples to evaluate (10000 for all samples in CIFAR-10)\ncfg_CORRUPTION_NUM_EX = 10000\n\n# ------------------------------- Batch norm options ------------------------ #\n#cfg_BN = CfgNode()\n\n# BN epsilon\ncfg_BN_EPS = 1e-5\n\n# BN momentum (BN momentum in PyTorch = 1 - BN momentum in Caffe2)\ncfg_BN_MOM = 0.1\n\n# ------------------------------- Optimizer options ------------------------- #\n#cfg_OPTIM = CfgNode()\n\n# Number of updates per batch\ncfg_OPTIM_STEPS = 1\n\n# Learning rate\ncfg_OPTIM_LR = 1e-3\n\n# Choices: Adam, SGD\ncfg_OPTIM_METHOD = 'Adam'\n\n# Beta\ncfg_OPTIM_BETA = 0.9\n\n# Momentum\ncfg_OPTIM_MOMENTUM = 0.9\n\n# Momentum dampening\ncfg_OPTIM_DAMPENING = 0.0\n\n# Nesterov momentum\ncfg_OPTIM_NESTEROV = True\n\n# L2 regularization\ncfg_OPTIM_WD = 0.0\n\n# ------------------------------- Testing options --------------------------- #\n#cfg_TEST = CfgNode()\n\n# Batch size for evaluation (and updates for norm + tent)\ncfg_TEST_BATCH_SIZE = 128\n\n# --------------------------------- CUDNN options --------------------------- #\n#cfg_CUDNN = CfgNode()\n\n# Benchmark to select fastest CUDNN algorithms (best for fixed input sizes)\ncfg_CUDNN_BENCHMARK = True\n\n# ---------------------------------- Misc options --------------------------- #\n\n# Optional description of a config\ncfg_DESC = \"\"\n\n# Note that non-determinism is still present due to non-deterministic GPU ops\ncfg_RNG_SEED = 1\n\n# Output directory\ncfg_SAVE_DIR = \"./output\"\n\n# Data directory\ncfg_DATA_DIR = \"./data\"\n\n# Weight directory\ncfg_CKPT_DIR = \"./ckpt\"\n\n# Log destination (in SAVE_DIR)\ncfg_LOG_DEST = \"log.txt\"\n\n# Log datetime\ncfg_LOG_TIME = ''","metadata":{"execution":{"iopub.status.busy":"2025-01-28T03:29:09.308057Z","iopub.execute_input":"2025-01-28T03:29:09.308305Z","iopub.status.idle":"2025-01-28T03:29:09.316121Z","shell.execute_reply.started":"2025-01-28T03:29:09.308283Z","shell.execute_reply":"2025-01-28T03:29:09.315260Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import logging\n\nimport torch\nimport torch.optim as optim\n\nfrom robustbench.data import load_cifar10c\nfrom robustbench.model_zoo.enums import ThreatModel\nfrom robustbench.utils import load_model\nfrom robustbench.utils import clean_accuracy as accuracy\n\n\nlogger = logging.getLogger(__name__)\n","metadata":{"execution":{"iopub.status.busy":"2025-01-28T03:29:09.317233Z","iopub.execute_input":"2025-01-28T03:29:09.317820Z","iopub.status.idle":"2025-01-28T03:29:12.568868Z","shell.execute_reply.started":"2025-01-28T03:29:09.317789Z","shell.execute_reply":"2025-01-28T03:29:12.567933Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef setup_source(model):\n    \"\"\"Set up the baseline source model without adaptation.\"\"\"\n    model.eval()\n    logger.info(f\"model for evaluation: %s\", model)\n    return model","metadata":{"execution":{"iopub.status.busy":"2025-01-28T03:29:12.570019Z","iopub.execute_input":"2025-01-28T03:29:12.570900Z","iopub.status.idle":"2025-01-28T03:29:12.575483Z","shell.execute_reply.started":"2025-01-28T03:29:12.570865Z","shell.execute_reply":"2025-01-28T03:29:12.574416Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def setup_tent(model):\n    \"\"\"Set up tent adaptation.\n\n    Configure the model for training + feature modulation by batch statistics,\n    collect the parameters for feature modulation by gradient optimization,\n    set up the optimizer, and then tent the model.\n    \"\"\"\n    model = configure_model(model)\n    params, param_names = collect_params(model)\n    optimizer = setup_optimizer(params)\n    tent_model = Tent(model, optimizer,\n                           steps=cfg_OPTIM_STEPS,\n                           episodic=cfg_MODEL_EPISODIC)\n    print(f\"model for adaptation: %s\", model)\n    print(f\"params for adaptation: %s\", param_names)\n    print(f\"optimizer for adaptation: %s\", optimizer)\n    return tent_model\n\n\ndef setup_optimizer(params):\n    \"\"\"Set up optimizer for tent adaptation.\n\n    Tent needs an optimizer for test-time entropy minimization.\n    In principle, tent could make use of any gradient optimizer.\n    In practice, we advise choosing Adam or SGD+momentum.\n    For optimization settings, we advise to use the settings from the end of\n    trainig, if known, or start with a low learning rate (like 0.001) if not.\n\n    For best results, try tuning the learning rate and batch size.\n    \"\"\"\n    if cfg_OPTIM_METHOD == 'Adam':\n        return optim.Adam(params,\n                    lr=cfg_OPTIM_LR,\n                    betas=(cfg_OPTIM_BETA, 0.999),\n                    weight_decay=cfg_OPTIM_WD)\n    elif cfg_OPTIM_METHOD == 'SGD':\n        return optim.SGD(params,\n                   lr=cfg_OPTIM_LR,\n                   momentum=cfg_OPTIM_MOMENTUM,\n                   dampening=cfg_OPTIM_DAMPENING,\n                   weight_decay=cfg_OPTIM_WD,\n                   nesterov=cfg_OPTIM_NESTEROV)\n    else:\n        raise NotImplementedError","metadata":{"execution":{"iopub.status.busy":"2025-01-28T03:29:12.576630Z","iopub.execute_input":"2025-01-28T03:29:12.576924Z","iopub.status.idle":"2025-01-28T03:29:12.589291Z","shell.execute_reply.started":"2025-01-28T03:29:12.576904Z","shell.execute_reply":"2025-01-28T03:29:12.588371Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def evaluate(description):\n    #load_cfg_fom_args(description)\n    # configure model\n    base_model = load_model(cfg_MODEL_ARCH, cfg_CKPT_DIR,\n                       cfg_CORRUPTION_DATASET, ThreatModel.corruptions).cuda()\n    if cfg_MODEL_ADAPTATION == \"source\":\n        logger.info(\"test-time adaptation: NONE\")\n        model = setup_source(base_model)\n    if cfg_MODEL_ADAPTATION == \"norm\":\n        logger.info(\"test-time adaptation: NORM\")\n        model = setup_norm(base_model)\n    if cfg_MODEL_ADAPTATION == \"tent\":\n        logger.info(\"test-time adaptation: TENT\")\n        model = setup_tent(base_model)\n    # evaluate on each severity and type of corruption in turn\n    print(\"done\")\n    for severity in cfg_CORRUPTION_SEVERITY:\n        for corruption_type in cfg_CORRUPTION_TYPE:\n            # reset adaptation for each combination of corruption x severity\n            # note: for evaluation protocol, but not necessarily needed\n            try:\n                model.reset()\n                print(\"resetting model\")\n            except:\n                logger.warning(\"not resetting model\")\n            x_test, y_test = load_cifar10c(cfg_CORRUPTION_NUM_EX,\n                                           severity, cfg_DATA_DIR, False,\n                                           [corruption_type])\n            x_test, y_test = x_test.cuda(), y_test.cuda()\n            #print(x_test.shape, y_test.shape)\n            acc = accuracy(model, x_test, y_test, cfg_TEST_BATCH_SIZE)\n            err = 1. - acc\n            print(corruption_type)\n            print(severity)\n            print(err)\n            #print(\"error % [{corruption_type}{severity}]: {err:.2%}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2025-01-28T03:29:12.590260Z","iopub.execute_input":"2025-01-28T03:29:12.590599Z","iopub.status.idle":"2025-01-28T03:29:12.603693Z","shell.execute_reply.started":"2025-01-28T03:29:12.590572Z","shell.execute_reply":"2025-01-28T03:29:12.602803Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"if __name__ == '__main__':\n    evaluate('\"CIFAR-10-C evaluation.')","metadata":{"execution":{"iopub.status.busy":"2025-01-28T03:29:12.604639Z","iopub.execute_input":"2025-01-28T03:29:12.604876Z","iopub.status.idle":"2025-01-28T03:45:29.308957Z","shell.execute_reply.started":"2025-01-28T03:29:12.604857Z","shell.execute_reply":"2025-01-28T03:45:29.308033Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Downloading ckpt/cifar10/corruptions/Standard.pt (gdrive_id=1t98aEuzeTL8P7Kpd5DIrCoCL21BNZUhC).\n","output_type":"stream"},{"name":"stderr","text":"Downloading...\nFrom (original): https://drive.google.com/uc?id=1t98aEuzeTL8P7Kpd5DIrCoCL21BNZUhC\nFrom (redirected): https://drive.google.com/uc?id=1t98aEuzeTL8P7Kpd5DIrCoCL21BNZUhC&confirm=t&uuid=3db333d5-3a48-406e-a16a-79c3f6362b5d\nTo: /kaggle/working/ckpt/cifar10/corruptions/Standard.pt\n100%|██████████| 292M/292M [00:03<00:00, 93.6MB/s] \n","output_type":"stream"},{"name":"stdout","text":"done\nStarting download from https://zenodo.org/api/records/2535967/files/CIFAR-10-C.tar/content\n","output_type":"stream"},{"name":"stderr","text":"44533it [03:40, 201.63it/s]                           \n","output_type":"stream"},{"name":"stdout","text":"Download finished, extracting...\nDownloaded and extracted.\ngaussian_noise\n5\n0.7233\nshot_noise\n5\n0.6571\nimpulse_noise\n5\n0.7292000000000001\ndefocus_blur\n5\n0.46940000000000004\nglass_blur\n5\n0.5432\nmotion_blur\n5\n0.34750000000000003\nzoom_blur\n5\n0.4202\nsnow\n5\n0.25070000000000003\nfrost\n5\n0.41300000000000003\nfog\n5\n0.2601\nbrightness\n5\n0.09299999999999997\ncontrast\n5\n0.4669\nelastic_transform\n5\n0.2659\npixelate\n5\n0.5845\njpeg_compression\n5\n0.30300000000000005\ngaussian_noise\n4\n0.6738999999999999\nshot_noise\n4\n0.5465\nimpulse_noise\n4\n0.5989\ndefocus_blur\n4\n0.22560000000000002\nglass_blur\n4\n0.5681\nmotion_blur\n4\n0.25170000000000003\nzoom_blur\n4\n0.29710000000000003\nsnow\n4\n0.19489999999999996\nfrost\n4\n0.2914\nfog\n4\n0.10409999999999997\nbrightness\n4\n0.07189999999999996\ncontrast\n4\n0.16410000000000002\nelastic_transform\n4\n0.2106\npixelate\n4\n0.39649999999999996\njpeg_compression\n4\n0.25849999999999995\ngaussian_noise\n3\n0.6081\nshot_noise\n3\n0.46509999999999996\nimpulse_noise\n3\n0.4263\ndefocus_blur\n3\n0.11019999999999996\nglass_blur\n3\n0.4305\nmotion_blur\n3\n0.256\nzoom_blur\n3\n0.22519999999999996\nsnow\n3\n0.16349999999999998\nfrost\n3\n0.27390000000000003\nfog\n3\n0.07709999999999995\nbrightness\n3\n0.06220000000000003\ncontrast\n3\n0.1028\nelastic_transform\n3\n0.14139999999999997\npixelate\n3\n0.19979999999999998\njpeg_compression\n3\n0.22040000000000004\ngaussian_noise\n2\n0.4212\nshot_noise\n2\n0.24270000000000003\nimpulse_noise\n2\n0.3095\ndefocus_blur\n2\n0.06620000000000004\nglass_blur\n2\n0.4425\nmotion_blur\n2\n0.16149999999999998\nzoom_blur\n2\n0.15449999999999997\nsnow\n2\n0.19569999999999999\nfrost\n2\n0.16500000000000004\nfog\n2\n0.06440000000000001\nbrightness\n2\n0.05620000000000003\ncontrast\n2\n0.07840000000000003\nelastic_transform\n2\n0.0978\npixelate\n2\n0.13959999999999995\njpeg_compression\n2\n0.20350000000000001\ngaussian_noise\n1\n0.22219999999999995\nshot_noise\n1\n0.14959999999999996\nimpulse_noise\n1\n0.17110000000000003\ndefocus_blur\n1\n0.054200000000000026\nglass_blur\n1\n0.4657\nmotion_blur\n1\n0.09709999999999996\nzoom_blur\n1\n0.12290000000000001\nsnow\n1\n0.10089999999999999\nfrost\n1\n0.10450000000000004\nfog\n1\n0.05489999999999995\nbrightness\n1\n0.05249999999999999\ncontrast\n1\n0.05730000000000002\nelastic_transform\n1\n0.09530000000000005\npixelate\n1\n0.08050000000000002\njpeg_compression\n1\n0.13639999999999997\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"cfg_MODEL_ADAPTATION = 'tent'\ncfg_TEST_BATCH_SIZE = 200\ncfg_OPTIM_METHOD = 'Adam'\ncfg_OPTIM_STEPS = 1\ncfg_OPTIM_BETA = 0.9\ncfg_OPTIM_LR = 1e-3\ncfg_OPTIM_WD = 0.\n","metadata":{"execution":{"iopub.status.busy":"2025-01-28T03:45:29.312001Z","iopub.execute_input":"2025-01-28T03:45:29.312300Z","iopub.status.idle":"2025-01-28T03:45:29.316627Z","shell.execute_reply.started":"2025-01-28T03:45:29.312277Z","shell.execute_reply":"2025-01-28T03:45:29.315678Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"if __name__ == '__main__':\n    evaluate('\"CIFAR-10-C evaluation.')","metadata":{"execution":{"iopub.status.busy":"2025-01-28T03:45:29.317802Z","iopub.execute_input":"2025-01-28T03:45:29.318116Z"},"trusted":true},"outputs":[{"name":"stdout","text":"model for adaptation: %s WideResNet(\n  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (block1): NetworkBlock(\n    (layer): Sequential(\n      (0): BasicBlock(\n        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(16, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (convShortcut): Conv2d(16, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      )\n      (1): BasicBlock(\n        (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (2): BasicBlock(\n        (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (3): BasicBlock(\n        (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n    )\n  )\n  (block2): NetworkBlock(\n    (layer): Sequential(\n      (0): BasicBlock(\n        (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (convShortcut): Conv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)\n      )\n      (1): BasicBlock(\n        (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (2): BasicBlock(\n        (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (3): BasicBlock(\n        (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n    )\n  )\n  (block3): NetworkBlock(\n    (layer): Sequential(\n      (0): BasicBlock(\n        (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (convShortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(2, 2), bias=False)\n      )\n      (1): BasicBlock(\n        (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (2): BasicBlock(\n        (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (3): BasicBlock(\n        (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n    )\n  )\n  (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n  (relu): ReLU(inplace=True)\n  (fc): Linear(in_features=640, out_features=10, bias=True)\n)\nparams for adaptation: %s ['block1.layer.0.bn1.weight', 'block1.layer.0.bn1.bias', 'block1.layer.0.bn2.weight', 'block1.layer.0.bn2.bias', 'block1.layer.1.bn1.weight', 'block1.layer.1.bn1.bias', 'block1.layer.1.bn2.weight', 'block1.layer.1.bn2.bias', 'block1.layer.2.bn1.weight', 'block1.layer.2.bn1.bias', 'block1.layer.2.bn2.weight', 'block1.layer.2.bn2.bias', 'block1.layer.3.bn1.weight', 'block1.layer.3.bn1.bias', 'block1.layer.3.bn2.weight', 'block1.layer.3.bn2.bias', 'block2.layer.0.bn1.weight', 'block2.layer.0.bn1.bias', 'block2.layer.0.bn2.weight', 'block2.layer.0.bn2.bias', 'block2.layer.1.bn1.weight', 'block2.layer.1.bn1.bias', 'block2.layer.1.bn2.weight', 'block2.layer.1.bn2.bias', 'block2.layer.2.bn1.weight', 'block2.layer.2.bn1.bias', 'block2.layer.2.bn2.weight', 'block2.layer.2.bn2.bias', 'block2.layer.3.bn1.weight', 'block2.layer.3.bn1.bias', 'block2.layer.3.bn2.weight', 'block2.layer.3.bn2.bias', 'block3.layer.0.bn1.weight', 'block3.layer.0.bn1.bias', 'block3.layer.0.bn2.weight', 'block3.layer.0.bn2.bias', 'block3.layer.1.bn1.weight', 'block3.layer.1.bn1.bias', 'block3.layer.1.bn2.weight', 'block3.layer.1.bn2.bias', 'block3.layer.2.bn1.weight', 'block3.layer.2.bn1.bias', 'block3.layer.2.bn2.weight', 'block3.layer.2.bn2.bias', 'block3.layer.3.bn1.weight', 'block3.layer.3.bn1.bias', 'block3.layer.3.bn2.weight', 'block3.layer.3.bn2.bias', 'bn1.weight', 'bn1.bias']\noptimizer for adaptation: %s Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.001\n    maximize: False\n    weight_decay: 0.0\n)\ndone\nresetting model\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}