{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2176821,"sourceType":"datasetVersion","datasetId":1306849}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms as T\nimport torchvision\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nfrom PIL import Image\nimport cv2\nimport albumentations as A\n\nimport time\nimport os\nfrom tqdm.notebook import tqdm\n\n!pip install -q segmentation-models-pytorch\n!pip install -q torchsummary\n\nfrom torchsummary import summary\nimport segmentation_models_pytorch as smp\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-11-18T22:50:41.462021Z","iopub.execute_input":"2023-11-18T22:50:41.462310Z","iopub.status.idle":"2023-11-18T22:51:27.925115Z","shell.execute_reply.started":"2023-11-18T22:50:41.462283Z","shell.execute_reply":"2023-11-18T22:51:27.924081Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"IMAGE_PATH = '/kaggle/input/the-hyper-kvasir-dataset/segmented-images/images/'\nMASK_PATH = '/kaggle/input/the-hyper-kvasir-dataset/segmented-images/masks/'","metadata":{"execution":{"iopub.status.busy":"2023-11-18T22:51:27.927165Z","iopub.execute_input":"2023-11-18T22:51:27.927550Z","iopub.status.idle":"2023-11-18T22:51:27.932208Z","shell.execute_reply.started":"2023-11-18T22:51:27.927517Z","shell.execute_reply":"2023-11-18T22:51:27.931281Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_classes = 2\n\ndef create_df():\n    name = []\n    for dirname, _, filenames in os.walk(IMAGE_PATH):\n        for filename in filenames:\n            name.append(filename.split('.')[0])\n    \n    return pd.DataFrame({'id': name}, index = np.arange(0, len(name)))\n\ndf = create_df()\nprint('Total Images: ', len(df))","metadata":{"execution":{"iopub.status.busy":"2023-11-18T22:51:27.933429Z","iopub.execute_input":"2023-11-18T22:51:27.933791Z","iopub.status.idle":"2023-11-18T22:51:28.973056Z","shell.execute_reply.started":"2023-11-18T22:51:27.933759Z","shell.execute_reply":"2023-11-18T22:51:28.972052Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#split data\nX_trainval, X_test = train_test_split(df['id'].values, test_size=0.1, random_state=19)\nX_train, X_val = train_test_split(X_trainval, test_size=0.15, random_state=19)\n\nprint('Train Size   : ', len(X_train))\nprint('Val Size     : ', len(X_val))\nprint('Test Size    : ', len(X_test))","metadata":{"execution":{"iopub.status.busy":"2023-11-18T22:51:28.975331Z","iopub.execute_input":"2023-11-18T22:51:28.975897Z","iopub.status.idle":"2023-11-18T22:51:28.991288Z","shell.execute_reply.started":"2023-11-18T22:51:28.975868Z","shell.execute_reply":"2023-11-18T22:51:28.990347Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img = Image.open(IMAGE_PATH + df['id'][100] + '.jpg')\nmask = Image.open(MASK_PATH + df['id'][100] + '.jpg')\nimport cv2\n\n# Load your RGB image\nrgb_image = cv2.imread(MASK_PATH + df['id'][100] + '.jpg')\n\n# Convert to grayscale using OpenCV\ngray_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2GRAY)\n\nimg=np.asarray(img)\n#mask= np.asarray(mask)/255\nprint('Image Size', np.asarray(img).shape)\nprint('Mask Size', np.asarray(mask).shape)\n\nprint('Mask Size', np.asarray(gray_image).shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-18T22:51:28.992651Z","iopub.execute_input":"2023-11-18T22:51:28.992927Z","iopub.status.idle":"2023-11-18T22:51:29.108780Z","shell.execute_reply.started":"2023-11-18T22:51:28.992902Z","shell.execute_reply":"2023-11-18T22:51:29.107852Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.imshow(img)\nplt.imshow(gray_image, alpha=0.6)\nplt.title('Picture with Mask Appplied')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-18T22:51:29.110024Z","iopub.execute_input":"2023-11-18T22:51:29.110339Z","iopub.status.idle":"2023-11-18T22:51:29.626068Z","shell.execute_reply.started":"2023-11-18T22:51:29.110314Z","shell.execute_reply":"2023-11-18T22:51:29.625138Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nmain_path = \"/kaggle/input/the-hyper-kvasir-dataset/segmented-images/\"\n\n\ndef get_bbox(json_file, ext=\".jpg\", nl=256, nc=256):\n    with open(json_file, \"r\") as fp:\n        data = json.load(fp)\n    bbox = {}\n    c=0\n    for key, value in data.items():\n#         c=c+1\n#         if c==1:\n#             print(key)\n#             print(value.shape)\n        bbox[key + ext] = np.array([value[\"bbox\"][0][\"xmin\"] * nl/value[\"width\"], \n                                   value[\"bbox\"][0][\"ymin\"] * nc/value[\"height\"], \n                                   value[\"bbox\"][0][\"xmax\"] * nl/value[\"width\"], # - value[\"bbox\"][0][\"xmin\"], \n                                   value[\"bbox\"][0][\"ymax\"] * nc/value[\"height\"]]) # - value[\"bbox\"][0][\"ymin\"])\n    return bbox\n\nbbox_file=os.path.join(main_path, \"bounding-boxes.json\")","metadata":{"execution":{"iopub.status.busy":"2023-11-18T22:51:29.627240Z","iopub.execute_input":"2023-11-18T22:51:29.627575Z","iopub.status.idle":"2023-11-18T22:51:29.635157Z","shell.execute_reply.started":"2023-11-18T22:51:29.627548Z","shell.execute_reply":"2023-11-18T22:51:29.634245Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#final one which also works for patches\nclass Dataset_create(Dataset):\n    \n    def __init__(self, img_path, mask_path, X, mean, std, transform=None, patch=False):\n        self.img_path = img_path\n        self.mask_path = mask_path\n        self.X = X\n        self.transform = transform\n        self.patches = patch\n        self.mean = mean\n        self.std = std\n        self.bboxes = get_bbox(bbox_file)\n        \n        \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        img = cv2.imread(self.img_path + self.X[idx] + '.jpg')\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(self.mask_path + self.X[idx] + '.jpg', cv2.IMREAD_GRAYSCALE)\n        bbox = self.bboxes[self.X[idx] + '.jpg']\n        \n        if self.transform is not None:\n            aug = self.transform(image=img, mask=mask)\n            img = Image.fromarray(aug['image'])\n            mask = aug['mask']\n        \n        if self.transform is None:\n            img = Image.fromarray(img)\n        \n       \n\n        t = T.Compose([T.ToTensor(), T.Normalize(self.mean, self.std)])\n        img = t(img)\n        \n        #extra to make 128,128 to 128,128,1\n        # Add an extra dimension to convert it to (128, 128, 1)\n        mask = mask[:, :, np.newaxis]\n        t2 = T.Compose([T.ToTensor()])\n        mask = t2(mask)\n        \n        \n        #print(mask.shape)\n        #mask = torch.from_numpy(mask).long()\n        #mask = mask.long()\n        \n        if self.patches:\n            img, mask = self.tiles(img, mask)\n            \n        return img, mask, bbox\n    \n    def tiles(self, img, mask):\n\n        img_patches = img.unfold(1,  64,64).unfold(2,  64,64) \n        #print(\"ok\")\n        img_patches  = img_patches.contiguous().view(3,-1,  64,64) \n        #print(\"ok2\")\n        img_patches = img_patches.permute(1,0,2,3)\n        #print(\"ok3\")\n        \n        mask_patches = mask.unfold(1,  64,64).unfold(2,  64,64)\n        mask_patches = mask_patches.contiguous().view(1,-1,  64,64)\n        mask_patches = mask_patches.permute(1,0,2,3)\n        \n        return img_patches, mask_patches","metadata":{"execution":{"iopub.status.busy":"2023-11-18T22:51:29.636485Z","iopub.execute_input":"2023-11-18T22:51:29.636984Z","iopub.status.idle":"2023-11-18T22:51:29.651595Z","shell.execute_reply.started":"2023-11-18T22:51:29.636952Z","shell.execute_reply":"2023-11-18T22:51:29.650592Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mean=[0.485, 0.456, 0.406]\nstd=[0.229, 0.224, 0.225]\n\nt_train = A.Compose([A.Resize(256,256, interpolation=cv2.INTER_NEAREST), A.HorizontalFlip(), A.VerticalFlip(), \n                     A.GridDistortion(p=0.2), A.RandomBrightnessContrast((0,0.5),(0,0.5)),\n                     A.GaussNoise()])\n\nt_val = A.Compose([A.Resize(256,256, interpolation=cv2.INTER_NEAREST), A.HorizontalFlip(),\n                   A.GridDistortion(p=0.2)])\n\n#datasets\ntrain_set = Dataset_create(IMAGE_PATH, MASK_PATH, X_train, mean, std, t_train, patch=False)\nval_set = Dataset_create(IMAGE_PATH, MASK_PATH, X_val, mean, std, t_val, patch=False)\n\n#dataloader\nbatch_size= 16\n\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)    ","metadata":{"execution":{"iopub.status.busy":"2023-11-18T22:51:29.652799Z","iopub.execute_input":"2023-11-18T22:51:29.653185Z","iopub.status.idle":"2023-11-18T22:51:29.697715Z","shell.execute_reply.started":"2023-11-18T22:51:29.653152Z","shell.execute_reply":"2023-11-18T22:51:29.696867Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\n\n\n# Assuming each batch contains image and mask tensors\nfor batch_idx, (images, masks, bbox) in enumerate(train_loader):\n    # Print the shape of the first batch\n    if batch_idx == 0:\n        print(\"Image batch shape:\", images.shape)\n        print(\"Mask batch shape:\", masks.shape)\n        print(bbox)\n    \n    # Break after the first batch if you only want to print one iteration\n    break\n","metadata":{"execution":{"iopub.status.busy":"2023-11-18T22:51:29.701523Z","iopub.execute_input":"2023-11-18T22:51:29.701799Z","iopub.status.idle":"2023-11-18T22:51:30.403039Z","shell.execute_reply.started":"2023-11-18T22:51:29.701776Z","shell.execute_reply":"2023-11-18T22:51:30.402140Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"im , mk,bbox = train_set[6]\nmyfiles=[im,mk]\nprint(mk.shape)\nprint(im.shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-18T22:51:30.404125Z","iopub.execute_input":"2023-11-18T22:51:30.404411Z","iopub.status.idle":"2023-11-18T22:51:30.433759Z","shell.execute_reply.started":"2023-11-18T22:51:30.404388Z","shell.execute_reply":"2023-11-18T22:51:30.432928Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# model + training","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os , glob\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport cv2\nfrom tqdm import tqdm\n\nimport torch\nfrom torch import Tensor\nfrom torch.autograd import Function\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom keras.preprocessing import image\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nfrom torch.utils.data import Dataset , DataLoader\nfrom torchvision import transforms , utils , datasets\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2023-11-18T22:51:30.434811Z","iopub.execute_input":"2023-11-18T22:51:30.435074Z","iopub.status.idle":"2023-11-18T22:51:44.187895Z","shell.execute_reply.started":"2023-11-18T22:51:30.435051Z","shell.execute_reply":"2023-11-18T22:51:44.187034Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DoubleConv(nn.Module):\n\n    def __init__(self, in_channels, out_channels, mid_channels=None):\n        \n        super().__init__()\n        \n        if not mid_channels:\n            mid_channels = out_channels\n        \n        self.double_conv = nn.Sequential(\n            \n            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(mid_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n    \n    \n    \nclass Down(nn.Module):\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        \n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(in_channels, out_channels)\n        )\n\n    def forward(self, x):\n        return self.maxpool_conv(x)\n    \n    \nclass Up(nn.Module):\n\n    def __init__(self, in_channels, out_channels, bilinear=True):\n        super().__init__()\n\n        # if bilinear, use the normal convolutions to reduce the number of channels\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n        else:\n            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n            self.conv = DoubleConv(in_channels, out_channels)\n\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        # input is CHW\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n\n        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n                        diffY // 2, diffY - diffY // 2])\n        # if you have padding issues, see\n        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)\n\n\nclass OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(OutConv, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        return self.conv(x)\n    \n    \n    \nclass UNet(nn.Module):\n    def __init__(self, n_channels, n_classes, bilinear=True):\n        super(UNet, self).__init__()\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.bilinear = bilinear\n\n        self.inc = DoubleConv(n_channels, 64)\n        self.down1 = Down(64, 128)\n        self.down2 = Down(128, 256)\n        self.down3 = Down(256, 512)\n        factor = 2 if bilinear else 1\n        self.down4 = Down(512, 1024 // factor)\n        self.up1 = Up(1024, 512 // factor, bilinear)\n        self.up2 = Up(512, 256 // factor, bilinear)\n        self.up3 = Up(256, 128 // factor, bilinear)\n        self.up4 = Up(128, 64, bilinear)\n        self.outc = OutConv(64, n_classes)\n\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        logits = self.outc(x)\n        return logits\n\ndef dice_calc(gt,pred) :\n    pred = torch.sigmoid(pred)\n    pred = ((pred) >= .5).float()\n    dice_score = (2 * (pred * gt).sum()) / ((pred + gt).sum() + 1e-8)\n    \n    return dice_score","metadata":{"execution":{"iopub.status.busy":"2023-11-18T22:51:44.189446Z","iopub.execute_input":"2023-11-18T22:51:44.190432Z","iopub.status.idle":"2023-11-18T22:51:44.213694Z","shell.execute_reply.started":"2023-11-18T22:51:44.190394Z","shell.execute_reply":"2023-11-18T22:51:44.212772Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"net=smp.Unet('efficientnet-b3', encoder_weights='imagenet', classes=1, activation=None, encoder_depth=5, decoder_channels=[ 256, 128, 64, 32, 16])","metadata":{"execution":{"iopub.status.busy":"2023-11-18T22:51:44.214825Z","iopub.execute_input":"2023-11-18T22:51:44.215113Z","iopub.status.idle":"2023-11-18T22:51:45.146686Z","shell.execute_reply.started":"2023-11-18T22:51:44.215087Z","shell.execute_reply":"2023-11-18T22:51:45.145877Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision      import datasets, models, transforms\n\nnet2 = models.resnet50(weights=\"IMAGENET1K_V2\")\nfc_in_size = net2.fc.in_features\nnet2.fc = nn.Linear(fc_in_size, 4)\n   ","metadata":{"execution":{"iopub.status.busy":"2023-11-18T22:51:45.147972Z","iopub.execute_input":"2023-11-18T22:51:45.148842Z","iopub.status.idle":"2023-11-18T22:51:46.203578Z","shell.execute_reply.started":"2023-11-18T22:51:45.148808Z","shell.execute_reply":"2023-11-18T22:51:46.202739Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define your model, optimizer, and loss criterion\ndevice = torch.device(\"cuda:0\")\n\nnet.to(device=device)\nnet2.to(device=device)\noptimizer = optim.RMSprop(net.parameters(), lr=0.0001, weight_decay=1e-8, momentum=0.9)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2)\ncriterion = nn.BCEWithLogitsLoss()\n\n#bbox\ncriterion2 = nn.SmoothL1Loss()\noptimizer2 = optim.Adam(net.parameters(), lr=1e-4)\n# if net.n_classes > 1:\n#     criterion = nn.CrossEntropyLoss()\n# else:\n#     criterion = nn.BCEWithLogitsLoss()","metadata":{"execution":{"iopub.status.busy":"2023-11-18T22:51:46.204880Z","iopub.execute_input":"2023-11-18T22:51:46.205228Z","iopub.status.idle":"2023-11-18T22:51:54.159535Z","shell.execute_reply.started":"2023-11-18T22:51:46.205197Z","shell.execute_reply":"2023-11-18T22:51:54.158650Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#this is the final one patch will also work\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport numpy as np\nfrom tqdm import tqdm\n\n# Define the Dice Score calculation function\ndef dice_calc(gt, pred):\n    pred = torch.sigmoid(pred)\n    pred = (pred >= 0.5).float()\n    intersection = (pred * gt).sum()\n    union = (pred + gt).sum() + 1e-8\n    dice_score = (2 * intersection) / union\n    return dice_score\n\n# Define the IoU calculation function\ndef iou_calc(gt, pred):\n    pred = torch.sigmoid(pred)\n    pred = (pred >= 0.5).float()\n    intersection = (pred * gt).sum()\n    union = (pred + gt).sum() - intersection + 1e-8\n    iou_score = intersection / union\n    return iou_score\n\n\n\n# Training loop\ndef train(epoch, epochs, tloader, patch=False):\n    net.train()\n    tloader.set_description(f'EPOCH {epoch}')\n    epoch_loss = 0\n    epoch_bbox_loss = 0\n    dice_score = 0\n    iou_score = 0\n    \n    for images, masks, bbox in tloader:\n        optimizer.zero_grad()\n        images = images.to(device, dtype=torch.float32)\n        masks = masks.to(device, dtype=torch.float32)\n        bbox = bbox.to(device)\n        if patch:\n            # Perform patch-wise training\n            batch_size, num_patches, num_channels, height, width = images.shape\n            images = images.view(-1, num_channels, height, width)\n            masks = masks.view(-1, 1, height, width)\n            \n            \n        #mask segmentation    \n        mask_pred = net(images)\n        loss = criterion(mask_pred, masks)\n        epoch_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n        \n        \n        #bbox\n        #bbox\n        bbox_predictions = net2(images)\n        loss2 = criterion2(bbox_predictions, bbox)\n        loss2.backward()\n        optimizer2.step()\n        epoch_bbox_loss += loss2.item()\n        \n        \n        \n        \n        \n        \n        running_DS = dice_calc(masks, mask_pred)\n        dice_score += running_DS\n        running_IOU = iou_calc(masks, mask_pred)\n        iou_score += running_IOU\n        tloader.set_postfix(loss=loss.item(), dice_score=running_DS.item(), iou_score=running_IOU.item())\n        \n        \n        \n    print(epoch_bbox_loss/len(tloader))\n    print(f'Train Dice Score Epoch: {dice_score / len(tloader)}')\n    print(f'Train IoU Score Epoch: {iou_score / len(tloader)}')\n\n# Validation loop\ndef validation(vloader, patch=False):\n    net.eval()\n    net2.eval()\n    vloader.set_description('Validation')\n    epoch_loss_bbox = 0\n    dice_score = 0\n    iou_score = 0\n    \n    with torch.no_grad():\n        for images, masks, bbox in vloader:\n            images = images.to(device)\n            masks = masks.to(device)\n            bbox = bbox.to(device)\n            if patch:\n                # Perform patch-wise validation\n                batch_size, num_patches, num_channels, height, width = images.shape\n                images = images.view(-1, num_channels, height, width)\n                masks = masks.view(-1, 1, height, width)\n                \n            #mask segmentation \n            mask_pred = net(images)\n            \n            \n            \n            #bbox\n            #convert to 1d tensor\n            predictions_bbox = net2(images)\n            #compute loss\n            loss_bbox = criterion2(predictions_bbox, bbox)\n            #keep track of loss\n            epoch_loss_bbox += loss_bbox.item()\n\n            \n            \n            \n            \n            running_DS = dice_calc(masks, mask_pred)\n            dice_score += running_DS\n            running_IOU = iou_calc(masks, mask_pred)\n            iou_score += running_IOU\n            \n            vloader.set_postfix(dice_score=running_DS.item(), iou_score=running_IOU.item())\n    \n    print(epoch_loss_bbox / len(vloader))\n    print(f'Validation Dice Score Epoch: {dice_score / len(vloader)}')\n    print(f'Validation IoU Score Epoch: {iou_score / len(vloader)}')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-18T22:57:14.658114Z","iopub.execute_input":"2023-11-18T22:57:14.658510Z","iopub.status.idle":"2023-11-18T22:57:14.678558Z","shell.execute_reply.started":"2023-11-18T22:57:14.658479Z","shell.execute_reply":"2023-11-18T22:57:14.677516Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = 10\n\nfor epoch in range(epochs) :\n    print(epoch+1,'/',epochs)\n    with tqdm(train_loader,unit='batch') as tloader : \n        train(epoch,epochs,tloader,patch=False)\n    \n    with tqdm(val_loader,unit='batch') as vloader:\n        validation(vloader,patch=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-18T22:57:14.936345Z","iopub.execute_input":"2023-11-18T22:57:14.937024Z","iopub.status.idle":"2023-11-18T22:59:03.330873Z","shell.execute_reply.started":"2023-11-18T22:57:14.936992Z","shell.execute_reply":"2023-11-18T22:59:03.329503Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# epochs = 40\n\n# for epoch in range(epochs) :\n#     print(epoch+1,'/',epochs)\n#     with tqdm(train_loader,unit='batch') as tloader : \n#         train(epoch,epochs,tloader)\n    \n#     with tqdm(val_loader,unit='batch') as vloader:\n#         validation(vloader)","metadata":{"execution":{"iopub.status.busy":"2023-11-18T22:52:07.814167Z","iopub.status.idle":"2023-11-18T22:52:07.814674Z","shell.execute_reply.started":"2023-11-18T22:52:07.814400Z","shell.execute_reply":"2023-11-18T22:52:07.814422Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#this will not work for patches\nwith torch.no_grad():\n    for images ,masks in val_loader :\n        images = images.to(device)\n        print(images.shape)\n        masks  = masks.to(device)\n\n        mask_pred = net(images)\n        \n        img = images.cpu().numpy() \n        masks = masks.cpu().numpy()\n        pred = mask_pred.cpu().numpy()\n        masks_2 = (pred > 0.005).astype(int)\n        \n        fig, axes = plt.subplots(1, 4, figsize=(20, 20))\n        \n        axes[0].imshow(img[0][0])\n        axes[0].set_title('Actual img')\n        \n        axes[1].imshow(masks[0][0])\n        axes[1].set_title('Ground Truth Mask')\n        \n        axes[2].imshow(pred[0][0])\n        axes[2].set_title('Prababilistic Mask')\n        \n        axes[3].imshow(masks_2[0][0])\n        axes[3].set_title('Probabilistic Mask threshold')\n        break","metadata":{"execution":{"iopub.status.busy":"2023-11-18T22:52:07.816306Z","iopub.status.idle":"2023-11-18T22:52:07.816754Z","shell.execute_reply.started":"2023-11-18T22:52:07.816533Z","shell.execute_reply":"2023-11-18T22:52:07.816553Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}